{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YX1hxqUQn47M"
   },
   "source": [
    "# DEMO: PyTorch/TPU ResNet18 on CIFAR Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QYOlFL-Sn5cq"
   },
   "source": [
    "<h3>  &nbsp;&nbsp;Use Colab Cloud TPU&nbsp;&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a></h3>\n",
    "\n",
    "* On the main menu, click Runtime and select **Change runtime type**. Set \"TPU\" as the hardware accelerator.\n",
    "* The cell below makes sure you have access to a TPU on Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hx4YVNHametU"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['XRT_TPU_CONFIG'] = \"tpu_worker;0;10.95.103.130:8470\"  # 1.5\n",
    "os.environ['XRT_TPU_CONFIG'] = \"tpu_worker;0;10.212.69.186:8470\"  # nightly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iMdPRFXIn_jH"
   },
   "outputs": [],
   "source": [
    "FLAGS = {}\n",
    "FLAGS['data_dir'] = \"/tmp/cifar\"\n",
    "FLAGS['batch_size'] = 128\n",
    "FLAGS['num_workers'] = 4\n",
    "FLAGS['learning_rate'] = 0.02\n",
    "FLAGS['momentum'] = 0.9\n",
    "FLAGS['num_epochs'] = 20\n",
    "FLAGS['num_cores'] = 8\n",
    "FLAGS['log_steps'] = 20\n",
    "FLAGS['metrics_debug'] = False  # print(' ', end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Micd3xZvoA-c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.debug.metrics as met\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.utils.utils as xu\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "  expansion = 1\n",
    "\n",
    "  def __init__(self, in_planes, planes, stride=1):\n",
    "    super(BasicBlock, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(\n",
    "        in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(planes)\n",
    "    self.conv2 = nn.Conv2d(\n",
    "        planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "    self.shortcut = nn.Sequential()\n",
    "    if stride != 1 or in_planes != self.expansion * planes:\n",
    "      self.shortcut = nn.Sequential(\n",
    "          nn.Conv2d(\n",
    "              in_planes,\n",
    "              self.expansion * planes,\n",
    "              kernel_size=1,\n",
    "              stride=stride,\n",
    "              bias=False), nn.BatchNorm2d(self.expansion * planes))\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = F.relu(self.bn1(self.conv1(x)))\n",
    "    out = self.bn2(self.conv2(out))\n",
    "    out += self.shortcut(x)\n",
    "    out = F.relu(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "  def __init__(self, block, num_blocks, num_classes=10):\n",
    "    super(ResNet, self).__init__()\n",
    "    self.in_planes = 64\n",
    "\n",
    "    self.conv1 = nn.Conv2d(\n",
    "        3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(64)\n",
    "    self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "    self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "    self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "    self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "    self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "  def _make_layer(self, block, planes, num_blocks, stride):\n",
    "    strides = [stride] + [1] * (num_blocks - 1)\n",
    "    layers = []\n",
    "    for stride in strides:\n",
    "      layers.append(block(self.in_planes, planes, stride))\n",
    "      self.in_planes = planes * block.expansion\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = F.relu(self.bn1(self.conv1(x)))\n",
    "    out = self.layer1(out)\n",
    "    out = self.layer2(out)\n",
    "    out = self.layer3(out)\n",
    "    out = self.layer4(out)\n",
    "    out = F.avg_pool2d(out, 4)\n",
    "    out = torch.flatten(out, 1)\n",
    "    out = self.linear(out)\n",
    "    return F.log_softmax(out, dim=1)\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "  return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8vMl96KLoCq8"
   },
   "outputs": [],
   "source": [
    "SERIAL_EXEC = xmp.MpSerialExecutor()\n",
    "# Only instantiate model weights once in memory.\n",
    "WRAPPED_MODEL = xmp.MpModelWrapper(ResNet18())\n",
    "\n",
    "def train_resnet18():\n",
    "  torch.manual_seed(1)\n",
    "\n",
    "  def get_dataset():\n",
    "    norm = transforms.Normalize(\n",
    "        mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        norm,\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        norm,\n",
    "    ])\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root=FLAGS['data_dir'],\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform_train)\n",
    "    test_dataset = datasets.CIFAR10(\n",
    "        root=FLAGS['data_dir'],\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform_test)\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "  \n",
    "  # Using the serial executor avoids multiple processes\n",
    "  # to download the same data.\n",
    "  train_dataset, test_dataset = SERIAL_EXEC.run(get_dataset)\n",
    "\n",
    "  train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "      train_dataset,\n",
    "      num_replicas=xm.xrt_world_size(),\n",
    "      rank=xm.get_ordinal(),\n",
    "      shuffle=True)\n",
    "  train_loader = torch.utils.data.DataLoader(\n",
    "      train_dataset,\n",
    "      batch_size=FLAGS['batch_size'],\n",
    "      sampler=train_sampler,\n",
    "      num_workers=FLAGS['num_workers'],\n",
    "      drop_last=True)\n",
    "  test_loader = torch.utils.data.DataLoader(\n",
    "      test_dataset,\n",
    "      batch_size=FLAGS['batch_size'],\n",
    "      shuffle=False,\n",
    "      num_workers=FLAGS['num_workers'],\n",
    "      drop_last=True)\n",
    "\n",
    "  # Scale learning rate to num cores\n",
    "  learning_rate = FLAGS['learning_rate'] * xm.xrt_world_size()\n",
    "\n",
    "  # Get loss function, optimizer, and model\n",
    "  device = xm.xla_device()\n",
    "  model = WRAPPED_MODEL.to(device)\n",
    "  optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                        momentum=FLAGS['momentum'], weight_decay=5e-4)\n",
    "  loss_fn = nn.NLLLoss()\n",
    "\n",
    "  def train_loop_fn(loader):\n",
    "    tracker = xm.RateTracker()\n",
    "    model.train()\n",
    "    for x, (data, target) in enumerate(loader):\n",
    "      optimizer.zero_grad()\n",
    "      output = model(data)\n",
    "      loss = loss_fn(output, target)\n",
    "      loss.backward()\n",
    "      xm.optimizer_step(optimizer)\n",
    "      tracker.add(FLAGS['batch_size'])\n",
    "      if x % FLAGS['log_steps'] == 0:\n",
    "        print('[xla:{}]({}) Loss={:.5f} Rate={:.2f} GlobalRate={:.2f} Time={}'.format(\n",
    "            xm.get_ordinal(), x, loss.item(), tracker.rate(),\n",
    "            tracker.global_rate(), time.asctime()), flush=True)\n",
    "\n",
    "  def test_loop_fn(loader):\n",
    "    total_samples = 0\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    data, pred, target = None, None, None\n",
    "    for data, target in loader:\n",
    "      output = model(data)\n",
    "      pred = output.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "      total_samples += data.size()[0]\n",
    "\n",
    "    accuracy = 100.0 * correct / total_samples\n",
    "    print('[xla:{}] Accuracy={:.2f}%'.format(\n",
    "        xm.get_ordinal(), accuracy), flush=True)\n",
    "    return accuracy, data, pred, target\n",
    "\n",
    "  # Train and eval loops\n",
    "  accuracy = 0.0\n",
    "  data, pred, target = None, None, None\n",
    "  for epoch in range(1, FLAGS['num_epochs'] + 1):\n",
    "    para_loader = pl.ParallelLoader(train_loader, [device])\n",
    "    train_loop_fn(para_loader.per_device_loader(device))\n",
    "    xm.master_print(\"Finished training epoch {}\".format(epoch))\n",
    "\n",
    "    para_loader = pl.ParallelLoader(test_loader, [device])\n",
    "    accuracy, data, pred, target  = test_loop_fn(para_loader.per_device_loader(device))\n",
    "    if FLAGS['metrics_debug']:\n",
    "      xm.master_print(met.metrics_report(), flush=True)\n",
    "\n",
    "  return accuracy, data, pred, target\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper to save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cMojPWZUqr2s"
   },
   "outputs": [],
   "source": [
    "# Result Visualization Helper\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "M, N = 4, 6\n",
    "RESULT_IMG_PATH = '/tmp/test_result.jpg'\n",
    "CIFAR10_LABELS = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "def plot_results(images, labels, preds):\n",
    "  images, labels, preds = images[:M*N], labels[:M*N], preds[:M*N]\n",
    "  inv_norm = transforms.Normalize(\n",
    "      mean=(-0.4914/0.2023, -0.4822/0.1994, -0.4465/0.2010),\n",
    "      std=(1/0.2023, 1/0.1994, 1/0.2010))\n",
    "\n",
    "  num_images = images.shape[0]\n",
    "  fig, axes = plt.subplots(M, N, figsize=(16, 9))\n",
    "  fig.suptitle('Correct / Predicted Labels (Red text for incorrect ones)')\n",
    "\n",
    "  for i, ax in enumerate(fig.axes):\n",
    "    ax.axis('off')\n",
    "    if i >= num_images:\n",
    "      continue\n",
    "    img, label, prediction = images[i], labels[i], preds[i]\n",
    "    img = inv_norm(img)\n",
    "    img = img.permute(1, 2, 0) # (C, M, N) -> (M, N, C)\n",
    "    label, prediction = label.item(), prediction.item()\n",
    "    if label == prediction:\n",
    "      ax.set_title(u'\\u2713', color='blue', fontsize=22)\n",
    "    else:\n",
    "      ax.set_title(\n",
    "          'X {}/{}'.format(CIFAR10_LABELS[label],\n",
    "                          CIFAR10_LABELS[prediction]), color='red')\n",
    "    ax.imshow(img)\n",
    "  plt.savefig(RESULT_IMG_PATH, transparent=True, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fire it up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_2nL4HmloEyl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[xla:0](0) Loss=2.37701 Rate=410.62 GlobalRate=410.60 Time=Thu Jul  9 18:58:09 2020\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[xla:7](0) Loss=2.37506 Rate=438.24 GlobalRate=438.23 Time=Thu Jul  9 18:58:10 2020\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[xla:1](0) Loss=2.37275 Rate=413.64 GlobalRate=413.62 Time=Thu Jul  9 18:58:12 2020\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[xla:5](0) Loss=2.35859 Rate=422.12 GlobalRate=422.09 Time=Thu Jul  9 18:58:14 2020\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[xla:6](0) Loss=2.32261 Rate=408.18 GlobalRate=408.17 Time=Thu Jul  9 18:58:16 2020\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[xla:4](0) Loss=2.46381 Rate=412.78 GlobalRate=412.76 Time=Thu Jul  9 18:58:17 2020\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[xla:2](0) Loss=2.38281 Rate=438.85 GlobalRate=438.83 Time=Thu Jul  9 18:58:19 2020\n",
      "Files already downloaded and verified\n",
      "[xla:3](0) Loss=2.34283 Rate=425.43 GlobalRate=425.41 Time=Thu Jul  9 18:58:21 2020\n",
      "[xla:5](20) Loss=2.25014 Rate=358.41 GlobalRate=319.77 Time=Thu Jul  9 18:58:22 2020\n",
      "[xla:0](20) Loss=2.00267 Rate=279.08 GlobalRate=196.38 Time=Thu Jul  9 18:58:22 2020\n",
      "[xla:3](20) Loss=2.09138 Rate=1754.43 GlobalRate=2115.83 Time=Thu Jul  9 18:58:22 2020\n",
      "[xla:2](20) Loss=2.10133 Rate=733.27 GlobalRate=882.55 Time=Thu Jul  9 18:58:22 2020\n",
      "[xla:7](20) Loss=2.13503 Rate=307.31 GlobalRate=225.36 Time=Thu Jul  9 18:58:22 2020\n",
      "[xla:6](20) Loss=2.36101 Rate=406.03 GlobalRate=404.76 Time=Thu Jul  9 18:58:22 2020\n",
      "[xla:1](20) Loss=2.36605 Rate=321.16 GlobalRate=264.19 Time=Thu Jul  9 18:58:22 2020\n",
      "[xla:4](20) Loss=2.09398 Rate=502.42 GlobalRate=552.65 Time=Thu Jul  9 18:58:22 2020\n",
      "[xla:4](40) Loss=1.91307 Rate=2565.74 GlobalRate=951.87 Time=Thu Jul  9 18:58:23 2020\n",
      "[xla:0](40) Loss=1.84218 Rate=2417.53 GlobalRate=365.61 Time=Thu Jul  9 18:58:23 2020\n",
      "[xla:7](40) Loss=1.89050 Rate=2428.03 GlobalRate=416.71 Time=Thu Jul  9 18:58:23 2020\n",
      "[xla:3](40) Loss=1.97613 Rate=3006.78 GlobalRate=2709.62 Time=Thu Jul  9 18:58:23 2020\n",
      "[xla:6](40) Loss=1.95461 Rate=2466.49 GlobalRate=718.16 Time=Thu Jul  9 18:58:23 2020\n",
      "[xla:5](40) Loss=1.84828 Rate=2447.20 GlobalRate=578.44 Time=Thu Jul  9 18:58:23 2020\n",
      "[xla:2](40) Loss=1.95638 Rate=2559.07 GlobalRate=1409.38 Time=Thu Jul  9 18:58:23 2020\n",
      "[xla:1](40) Loss=1.92111 Rate=2398.50 GlobalRate=483.64 Time=Thu Jul  9 18:58:23 2020\n",
      "Finished training epoch 1\n",
      "[xla:0] Accuracy=29.74%\n",
      "[xla:3] Accuracy=29.84%\n",
      "[xla:6] Accuracy=30.01%\n",
      "[xla:1] Accuracy=30.06%\n",
      "[xla:4] Accuracy=29.69%\n",
      "[xla:5] Accuracy=29.84%\n",
      "[xla:2] Accuracy=29.47%\n",
      "[xla:7] Accuracy=29.85%\n",
      "[xla:3](0) Loss=1.94690 Rate=245.77 GlobalRate=245.76 Time=Thu Jul  9 18:58:25 2020\n",
      "[xla:0](0) Loss=1.91019 Rate=231.74 GlobalRate=231.73 Time=Thu Jul  9 18:58:25 2020\n",
      "[xla:1](0) Loss=2.01098 Rate=225.95 GlobalRate=225.94 Time=Thu Jul  9 18:58:25 2020\n",
      "[xla:6](0) Loss=1.83501 Rate=195.70 GlobalRate=195.69 Time=Thu Jul  9 18:58:25 2020\n",
      "[xla:5](0) Loss=1.73513 Rate=196.39 GlobalRate=196.38 Time=Thu Jul  9 18:58:26 2020\n",
      "[xla:4](0) Loss=1.78981 Rate=211.69 GlobalRate=211.69 Time=Thu Jul  9 18:58:26 2020\n",
      "[xla:2](0) Loss=1.78732 Rate=195.73 GlobalRate=195.72 Time=Thu Jul  9 18:58:26 2020\n",
      "[xla:7](0) Loss=2.28302 Rate=205.44 GlobalRate=205.44 Time=Thu Jul  9 18:58:26 2020\n",
      "[xla:5](20) Loss=1.80075 Rate=1718.31 GlobalRate=1692.15 Time=Thu Jul  9 18:58:26 2020\n",
      "[xla:6](20) Loss=1.68114 Rate=1605.21 GlobalRate=1619.27 Time=Thu Jul  9 18:58:26 2020\n",
      "[xla:4](20) Loss=1.87455 Rate=1784.08 GlobalRate=1781.90 Time=Thu Jul  9 18:58:26 2020\n",
      "[xla:1](20) Loss=1.94715 Rate=1591.06 GlobalRate=1690.51 Time=Thu Jul  9 18:58:26 2020\n",
      "[xla:7](20) Loss=1.90962 Rate=1883.07 GlobalRate=1821.17 Time=Thu Jul  9 18:58:26 2020\n",
      "[xla:3](20) Loss=1.75630 Rate=1425.33 GlobalRate=1601.61 Time=Thu Jul  9 18:58:26 2020\n",
      "[xla:2](20) Loss=1.80554 Rate=1847.54 GlobalRate=1765.92 Time=Thu Jul  9 18:58:26 2020\n",
      "[xla:0](20) Loss=1.64946 Rate=1424.87 GlobalRate=1576.20 Time=Thu Jul  9 18:58:26 2020\n",
      "[xla:0](40) Loss=1.54487 Rate=2782.72 GlobalRate=2187.10 Time=Thu Jul  9 18:58:27 2020\n",
      "[xla:5](40) Loss=1.62824 Rate=2895.67 GlobalRate=2297.66 Time=Thu Jul  9 18:58:27 2020\n",
      "[xla:2](40) Loss=1.68839 Rate=2945.87 GlobalRate=2365.92 Time=Thu Jul  9 18:58:27 2020\n",
      "[xla:1](40) Loss=1.58652 Rate=2842.85 GlobalRate=2295.51 Time=Thu Jul  9 18:58:27 2020\n",
      "[xla:7](40) Loss=1.62725 Rate=2959.54 GlobalRate=2416.03 Time=Thu Jul  9 18:58:27 2020\n",
      "[xla:6](40) Loss=1.69266 Rate=2848.28 GlobalRate=2227.29 Time=Thu Jul  9 18:58:27 2020\n",
      "[xla:3](40) Loss=1.81911 Rate=2776.95 GlobalRate=2210.31 Time=Thu Jul  9 18:58:27 2020\n",
      "[xla:4](40) Loss=1.66838 Rate=2919.89 GlobalRate=2380.36 Time=Thu Jul  9 18:58:27 2020\n",
      "Finished training epoch 2\n",
      "[xla:0] Accuracy=37.13%\n",
      "[xla:3] Accuracy=36.99%\n",
      "[xla:2] Accuracy=37.02%\n",
      "[xla:1] Accuracy=37.14%\n",
      "[xla:4] Accuracy=37.08%\n",
      "[xla:6] Accuracy=36.98%\n",
      "[xla:5] Accuracy=36.86%\n",
      "[xla:7] Accuracy=37.47%\n",
      "[xla:0](0) Loss=1.61859 Rate=237.60 GlobalRate=237.58 Time=Thu Jul  9 18:58:30 2020\n",
      "[xla:3](0) Loss=1.65738 Rate=219.54 GlobalRate=219.54 Time=Thu Jul  9 18:58:30 2020\n",
      "[xla:4](0) Loss=1.60918 Rate=238.24 GlobalRate=238.23 Time=Thu Jul  9 18:58:30 2020\n",
      "[xla:1](0) Loss=1.70189 Rate=218.10 GlobalRate=218.09 Time=Thu Jul  9 18:58:30 2020\n",
      "[xla:2](0) Loss=1.56961 Rate=203.44 GlobalRate=203.43 Time=Thu Jul  9 18:58:30 2020\n",
      "[xla:6](0) Loss=1.60574 Rate=202.66 GlobalRate=202.65 Time=Thu Jul  9 18:58:30 2020\n",
      "[xla:5](0) Loss=1.58899 Rate=167.66 GlobalRate=167.65 Time=Thu Jul  9 18:58:30 2020\n",
      "[xla:7](0) Loss=1.70857 Rate=194.00 GlobalRate=193.99 Time=Thu Jul  9 18:58:30 2020\n",
      "[xla:4](20) Loss=1.73558 Rate=1466.07 GlobalRate=1621.40 Time=Thu Jul  9 18:58:31 2020\n",
      "[xla:0](20) Loss=1.51871 Rate=1379.52 GlobalRate=1549.64 Time=Thu Jul  9 18:58:31 2020\n",
      "[xla:3](20) Loss=1.59572 Rate=1449.24 GlobalRate=1570.76 Time=Thu Jul  9 18:58:31 2020\n",
      "[xla:1](20) Loss=1.79320 Rate=1485.78 GlobalRate=1595.07 Time=Thu Jul  9 18:58:31 2020\n",
      "[xla:2](20) Loss=1.59387 Rate=1521.78 GlobalRate=1585.30 Time=Thu Jul  9 18:58:31 2020\n",
      "[xla:5](20) Loss=1.56265 Rate=1780.61 GlobalRate=1619.41 Time=Thu Jul  9 18:58:31 2020\n",
      "[xla:6](20) Loss=1.57219 Rate=1565.10 GlobalRate=1612.83 Time=Thu Jul  9 18:58:31 2020\n",
      "[xla:7](20) Loss=1.72805 Rate=1911.42 GlobalRate=1795.09 Time=Thu Jul  9 18:58:31 2020\n",
      "[xla:2](40) Loss=1.56727 Rate=2884.32 GlobalRate=2213.83 Time=Thu Jul  9 18:58:32 2020\n",
      "[xla:4](40) Loss=1.49013 Rate=2860.58 GlobalRate=2249.23 Time=Thu Jul  9 18:58:32 2020\n",
      "[xla:5](40) Loss=1.49338 Rate=2989.32 GlobalRate=2248.10 Time=Thu Jul  9 18:58:32 2020\n",
      "[xla:1](40) Loss=1.43067 Rate=2869.73 GlobalRate=2223.51 Time=Thu Jul  9 18:58:32 2020\n",
      "[xla:6](40) Loss=1.50287 Rate=2905.66 GlobalRate=2242.32 Time=Thu Jul  9 18:58:32 2020\n",
      "[xla:7](40) Loss=1.55107 Rate=3062.47 GlobalRate=2423.07 Time=Thu Jul  9 18:58:32 2020\n",
      "[xla:3](40) Loss=1.63722 Rate=2854.78 GlobalRate=2199.12 Time=Thu Jul  9 18:58:32 2020\n",
      "[xla:0](40) Loss=1.37203 Rate=2827.19 GlobalRate=2177.94 Time=Thu Jul  9 18:58:32 2020\n",
      "Finished training epoch 3\n",
      "[xla:0] Accuracy=44.72%\n",
      "[xla:1] Accuracy=44.71%\n",
      "[xla:4] Accuracy=44.73%\n",
      "[xla:5] Accuracy=44.45%\n",
      "[xla:6] Accuracy=44.52%\n",
      "[xla:3] Accuracy=44.32%\n",
      "[xla:2] Accuracy=44.74%\n",
      "[xla:7] Accuracy=45.03%\n",
      "[xla:1](0) Loss=1.56612 Rate=251.64 GlobalRate=251.63 Time=Thu Jul  9 18:58:34 2020\n",
      "[xla:0](0) Loss=1.60636 Rate=238.75 GlobalRate=238.73 Time=Thu Jul  9 18:58:34 2020\n",
      "[xla:4](0) Loss=1.61820 Rate=215.85 GlobalRate=215.84 Time=Thu Jul  9 18:58:35 2020\n",
      "[xla:2](0) Loss=1.39828 Rate=213.02 GlobalRate=213.01 Time=Thu Jul  9 18:58:35 2020\n",
      "[xla:6](0) Loss=1.50592 Rate=185.06 GlobalRate=185.06 Time=Thu Jul  9 18:58:35 2020\n",
      "[xla:3](0) Loss=1.42845 Rate=183.88 GlobalRate=183.87 Time=Thu Jul  9 18:58:35 2020\n",
      "[xla:5](0) Loss=1.38644 Rate=179.44 GlobalRate=179.44 Time=Thu Jul  9 18:58:35 2020\n",
      "[xla:7](0) Loss=1.56675 Rate=178.20 GlobalRate=178.19 Time=Thu Jul  9 18:58:35 2020\n",
      "[xla:6](20) Loss=1.42247 Rate=1727.88 GlobalRate=1658.84 Time=Thu Jul  9 18:58:36 2020\n",
      "[xla:2](20) Loss=1.37171 Rate=1622.28 GlobalRate=1679.79 Time=Thu Jul  9 18:58:36 2020\n",
      "[xla:1](20) Loss=1.64313 Rate=1460.58 GlobalRate=1640.87 Time=Thu Jul  9 18:58:36 2020\n",
      "[xla:4](20) Loss=1.52908 Rate=1538.19 GlobalRate=1628.12 Time=Thu Jul  9 18:58:36 2020\n",
      "[xla:3](20) Loss=1.46151 Rate=1755.87 GlobalRate=1670.43 Time=Thu Jul  9 18:58:36 2020\n",
      "[xla:7](20) Loss=1.54932 Rate=1876.13 GlobalRate=1712.79 Time=Thu Jul  9 18:58:36 2020\n",
      "[xla:5](20) Loss=1.48136 Rate=1758.42 GlobalRate=1655.14 Time=Thu Jul  9 18:58:36 2020\n",
      "[xla:0](20) Loss=1.40637 Rate=1477.86 GlobalRate=1631.77 Time=Thu Jul  9 18:58:36 2020\n",
      "[xla:0](40) Loss=1.29216 Rate=2821.48 GlobalRate=2246.60 Time=Thu Jul  9 18:58:36 2020\n",
      "[xla:4](40) Loss=1.37641 Rate=2845.83 GlobalRate=2243.12 Time=Thu Jul  9 18:58:36 2020\n",
      "[xla:2](40) Loss=1.40484 Rate=2878.18 GlobalRate=2292.49 Time=Thu Jul  9 18:58:36 2020\n",
      "[xla:7](40) Loss=1.44174 Rate=2984.25 GlobalRate=2325.22 Time=Thu Jul  9 18:58:36 2020\n",
      "[xla:3](40) Loss=1.48208 Rate=2929.89 GlobalRate=2283.02 Time=Thu Jul  9 18:58:36 2020\n",
      "[xla:6](40) Loss=1.44085 Rate=2919.19 GlobalRate=2272.05 Time=Thu Jul  9 18:58:36 2020\n",
      "[xla:1](40) Loss=1.36615 Rate=2811.49 GlobalRate=2254.50 Time=Thu Jul  9 18:58:36 2020\n",
      "[xla:5](40) Loss=1.43977 Rate=2932.41 GlobalRate=2268.80 Time=Thu Jul  9 18:58:36 2020\n",
      "Finished training epoch 4\n",
      "[xla:1] Accuracy=47.72%\n",
      "[xla:5] Accuracy=47.03%\n",
      "[xla:4] Accuracy=47.30%\n",
      "[xla:6] Accuracy=47.12%\n",
      "[xla:7] Accuracy=47.50%\n",
      "[xla:0] Accuracy=47.37%\n",
      "[xla:3] Accuracy=47.12%\n",
      "[xla:2] Accuracy=47.57%\n",
      "[xla:5](0) Loss=1.23925 Rate=283.18 GlobalRate=283.16 Time=Thu Jul  9 18:58:39 2020\n",
      "[xla:1](0) Loss=1.44080 Rate=218.08 GlobalRate=218.08 Time=Thu Jul  9 18:58:39 2020\n",
      "[xla:3](0) Loss=1.33093 Rate=239.45 GlobalRate=239.44 Time=Thu Jul  9 18:58:39 2020\n",
      "[xla:4](0) Loss=1.39147 Rate=186.83 GlobalRate=186.83 Time=Thu Jul  9 18:58:39 2020\n",
      "[xla:7](0) Loss=1.46989 Rate=187.59 GlobalRate=187.58 Time=Thu Jul  9 18:58:39 2020\n",
      "[xla:2](0) Loss=1.30681 Rate=220.96 GlobalRate=220.95 Time=Thu Jul  9 18:58:39 2020\n",
      "[xla:6](0) Loss=1.33281 Rate=173.02 GlobalRate=173.01 Time=Thu Jul  9 18:58:39 2020\n",
      "[xla:0](0) Loss=1.28303 Rate=159.95 GlobalRate=159.95 Time=Thu Jul  9 18:58:39 2020\n",
      "[xla:1](20) Loss=1.50363 Rate=1370.00 GlobalRate=1506.43 Time=Thu Jul  9 18:58:40 2020\n",
      "[xla:4](20) Loss=1.47925 Rate=1508.83 GlobalRate=1530.60 Time=Thu Jul  9 18:58:40 2020\n",
      "[xla:0](20) Loss=1.23813 Rate=1798.73 GlobalRate=1594.60 Time=Thu Jul  9 18:58:40 2020\n",
      "[xla:7](20) Loss=1.41744 Rate=1542.67 GlobalRate=1554.70 Time=Thu Jul  9 18:58:40 2020\n",
      "[xla:2](20) Loss=1.27522 Rate=1627.28 GlobalRate=1704.04 Time=Thu Jul  9 18:58:40 2020\n",
      "[xla:3](20) Loss=1.33685 Rate=1503.43 GlobalRate=1653.39 Time=Thu Jul  9 18:58:40 2020\n",
      "[xla:5](20) Loss=1.35814 Rate=1267.28 GlobalRate=1507.53 Time=Thu Jul  9 18:58:40 2020\n",
      "[xla:6](20) Loss=1.30811 Rate=1611.18 GlobalRate=1548.44 Time=Thu Jul  9 18:58:40 2020\n",
      "[xla:4](40) Loss=1.29460 Rate=2959.33 GlobalRate=2179.24 Time=Thu Jul  9 18:58:41 2020\n",
      "[xla:3](40) Loss=1.35912 Rate=2956.56 GlobalRate=2303.85 Time=Thu Jul  9 18:58:41 2020\n",
      "[xla:7](40) Loss=1.27707 Rate=2972.22 GlobalRate=2203.99 Time=Thu Jul  9 18:58:41 2020\n",
      "[xla:0](40) Loss=1.15823 Rate=3073.01 GlobalRate=2244.34 Time=Thu Jul  9 18:58:41 2020\n",
      "[xla:2](40) Loss=1.24205 Rate=3005.99 GlobalRate=2353.75 Time=Thu Jul  9 18:58:41 2020\n",
      "[xla:1](40) Loss=1.21282 Rate=2900.90 GlobalRate=2153.32 Time=Thu Jul  9 18:58:41 2020\n",
      "[xla:6](40) Loss=1.29800 Rate=2999.86 GlobalRate=2197.60 Time=Thu Jul  9 18:58:41 2020\n",
      "[xla:5](40) Loss=1.37537 Rate=2861.36 GlobalRate=2154.86 Time=Thu Jul  9 18:58:41 2020\n",
      "Finished training epoch 5\n",
      "[xla:6] Accuracy=53.39%\n",
      "[xla:5] Accuracy=53.30%\n",
      "[xla:7] Accuracy=53.32%\n",
      "[xla:1] Accuracy=53.60%\n",
      "[xla:4] Accuracy=53.15%\n",
      "[xla:2] Accuracy=53.63%\n",
      "[xla:0] Accuracy=53.07%\n",
      "[xla:3] Accuracy=53.31%\n",
      "[xla:3](0) Loss=1.27368 Rate=240.09 GlobalRate=240.08 Time=Thu Jul  9 18:58:44 2020\n",
      "[xla:5](0) Loss=1.14822 Rate=215.37 GlobalRate=215.35 Time=Thu Jul  9 18:58:44 2020\n",
      "[xla:6](0) Loss=1.17428 Rate=202.16 GlobalRate=202.15 Time=Thu Jul  9 18:58:44 2020\n",
      "[xla:2](0) Loss=1.10428 Rate=218.73 GlobalRate=218.72 Time=Thu Jul  9 18:58:44 2020\n",
      "[xla:1](0) Loss=1.25403 Rate=205.33 GlobalRate=205.33 Time=Thu Jul  9 18:58:44 2020\n",
      "[xla:7](0) Loss=1.20623 Rate=199.61 GlobalRate=199.59 Time=Thu Jul  9 18:58:44 2020\n",
      "[xla:0](0) Loss=1.17696 Rate=198.55 GlobalRate=198.54 Time=Thu Jul  9 18:58:44 2020\n",
      "[xla:4](0) Loss=1.31129 Rate=200.15 GlobalRate=200.15 Time=Thu Jul  9 18:58:44 2020\n",
      "[xla:0](20) Loss=1.05588 Rate=1771.55 GlobalRate=1731.50 Time=Thu Jul  9 18:58:45 2020\n",
      "[xla:1](20) Loss=1.29437 Rate=1714.08 GlobalRate=1718.01 Time=Thu Jul  9 18:58:45 2020\n",
      "[xla:4](20) Loss=1.26782 Rate=1757.45 GlobalRate=1728.36 Time=Thu Jul  9 18:58:45 2020\n",
      "[xla:5](20) Loss=1.20719 Rate=1630.02 GlobalRate=1691.35 Time=Thu Jul  9 18:58:45 2020\n",
      "[xla:2](20) Loss=1.11958 Rate=1670.94 GlobalRate=1728.33 Time=Thu Jul  9 18:58:45 2020\n",
      "[xla:6](20) Loss=1.07867 Rate=1672.41 GlobalRate=1681.76 Time=Thu Jul  9 18:58:45 2020\n",
      "[xla:3](20) Loss=1.14271 Rate=1630.35 GlobalRate=1752.00 Time=Thu Jul  9 18:58:45 2020\n",
      "[xla:7](20) Loss=1.24685 Rate=1719.73 GlobalRate=1703.49 Time=Thu Jul  9 18:58:45 2020\n",
      "[xla:2](40) Loss=1.18082 Rate=2912.18 GlobalRate=2343.06 Time=Thu Jul  9 18:58:45 2020\n",
      "[xla:0](40) Loss=1.13865 Rate=2949.10 GlobalRate=2344.96 Time=Thu Jul  9 18:58:45 2020\n",
      "[xla:4](40) Loss=1.27502 Rate=2945.59 GlobalRate=2342.71 Time=Thu Jul  9 18:58:45 2020\n",
      "[xla:6](40) Loss=1.18207 Rate=2913.10 GlobalRate=2298.99 Time=Thu Jul  9 18:58:45 2020\n",
      "[xla:5](40) Loss=1.16912 Rate=2895.46 GlobalRate=2307.91 Time=Thu Jul  9 18:58:45 2020\n",
      "[xla:7](40) Loss=1.09046 Rate=2931.52 GlobalRate=2319.52 Time=Thu Jul  9 18:58:45 2020\n",
      "[xla:1](40) Loss=1.14847 Rate=2926.89 GlobalRate=2332.52 Time=Thu Jul  9 18:58:45 2020\n",
      "[xla:3](40) Loss=1.20170 Rate=2895.47 GlobalRate=2365.10 Time=Thu Jul  9 18:58:45 2020\n",
      "Finished training epoch 6\n",
      "[xla:4] Accuracy=52.18%\n",
      "[xla:3] Accuracy=52.68%\n",
      "[xla:6] Accuracy=52.46%\n",
      "[xla:7] Accuracy=52.28%\n",
      "[xla:5] Accuracy=52.43%\n",
      "[xla:0] Accuracy=52.75%\n",
      "[xla:2] Accuracy=52.51%\n",
      "[xla:1] Accuracy=53.11%\n",
      "[xla:2](0) Loss=0.97170 Rate=230.99 GlobalRate=230.98 Time=Thu Jul  9 18:58:48 2020\n",
      "[xla:0](0) Loss=1.06847 Rate=231.55 GlobalRate=231.54 Time=Thu Jul  9 18:58:48 2020\n",
      "[xla:4](0) Loss=1.22705 Rate=193.15 GlobalRate=193.14 Time=Thu Jul  9 18:58:48 2020\n",
      "[xla:6](0) Loss=1.09194 Rate=206.54 GlobalRate=206.53 Time=Thu Jul  9 18:58:48 2020\n",
      "[xla:1](0) Loss=1.12588 Rate=203.73 GlobalRate=203.72 Time=Thu Jul  9 18:58:48 2020\n",
      "[xla:3](0) Loss=1.08886 Rate=200.41 GlobalRate=200.41 Time=Thu Jul  9 18:58:48 2020\n",
      "[xla:5](0) Loss=0.92329 Rate=196.16 GlobalRate=196.16 Time=Thu Jul  9 18:58:48 2020\n",
      "[xla:7](0) Loss=1.07451 Rate=188.11 GlobalRate=188.10 Time=Thu Jul  9 18:58:48 2020\n",
      "[xla:2](20) Loss=0.94814 Rate=1617.55 GlobalRate=1721.69 Time=Thu Jul  9 18:58:49 2020\n",
      "[xla:4](20) Loss=1.11033 Rate=1698.25 GlobalRate=1669.27 Time=Thu Jul  9 18:58:49 2020\n",
      "[xla:7](20) Loss=1.15672 Rate=1786.67 GlobalRate=1703.45 Time=Thu Jul  9 18:58:49 2020\n",
      "[xla:0](20) Loss=0.87903 Rate=1633.28 GlobalRate=1734.43 Time=Thu Jul  9 18:58:49 2020\n",
      "[xla:3](20) Loss=0.97153 Rate=1725.79 GlobalRate=1709.84 Time=Thu Jul  9 18:58:49 2020\n",
      "[xla:5](20) Loss=1.12026 Rate=1738.54 GlobalRate=1703.64 Time=Thu Jul  9 18:58:49 2020\n",
      "[xla:1](20) Loss=1.10697 Rate=1729.19 GlobalRate=1722.53 Time=Thu Jul  9 18:58:49 2020\n",
      "[xla:6](20) Loss=0.95766 Rate=1676.59 GlobalRate=1697.63 Time=Thu Jul  9 18:58:49 2020\n",
      "[xla:3](40) Loss=1.01160 Rate=2929.91 GlobalRate=2324.27 Time=Thu Jul  9 18:58:50 2020\n",
      "[xla:5](40) Loss=1.08878 Rate=2936.98 GlobalRate=2319.01 Time=Thu Jul  9 18:58:50 2020\n",
      "[xla:0](40) Loss=0.95648 Rate=2894.03 GlobalRate=2347.80 Time=Thu Jul  9 18:58:50 2020\n",
      "[xla:4](40) Loss=1.16862 Rate=2918.94 GlobalRate=2285.61 Time=Thu Jul  9 18:58:50 2020\n",
      "[xla:6](40) Loss=1.01698 Rate=2958.56 GlobalRate=2327.53 Time=Thu Jul  9 18:58:50 2020\n",
      "[xla:7](40) Loss=0.91257 Rate=2955.05 GlobalRate=2318.46 Time=Thu Jul  9 18:58:50 2020\n",
      "[xla:2](40) Loss=1.07481 Rate=2884.09 GlobalRate=2334.63 Time=Thu Jul  9 18:58:50 2020\n",
      "[xla:1](40) Loss=1.09973 Rate=2932.29 GlobalRate=2336.58 Time=Thu Jul  9 18:58:50 2020\n",
      "Finished training epoch 7\n",
      "[xla:6] Accuracy=53.71%\n",
      "[xla:0] Accuracy=54.64%\n",
      "[xla:7] Accuracy=54.16%\n",
      "[xla:1] Accuracy=53.83%\n",
      "[xla:4] Accuracy=53.89%\n",
      "[xla:3] Accuracy=53.88%\n",
      "[xla:5] Accuracy=53.47%\n",
      "[xla:2] Accuracy=54.12%\n",
      "[xla:6](0) Loss=1.00387 Rate=351.49 GlobalRate=351.48 Time=Thu Jul  9 18:58:52 2020\n",
      "[xla:4](0) Loss=0.96734 Rate=222.86 GlobalRate=222.85 Time=Thu Jul  9 18:58:53 2020\n",
      "[xla:7](0) Loss=0.91401 Rate=205.74 GlobalRate=205.73 Time=Thu Jul  9 18:58:53 2020\n",
      "[xla:0](0) Loss=0.92006 Rate=194.24 GlobalRate=194.24 Time=Thu Jul  9 18:58:53 2020\n",
      "[xla:3](0) Loss=0.90468 Rate=204.39 GlobalRate=204.38 Time=Thu Jul  9 18:58:53 2020\n",
      "[xla:5](0) Loss=0.91452 Rate=193.00 GlobalRate=193.00 Time=Thu Jul  9 18:58:53 2020\n",
      "[xla:1](0) Loss=0.99301 Rate=173.84 GlobalRate=173.84 Time=Thu Jul  9 18:58:53 2020\n",
      "[xla:2](0) Loss=0.85750 Rate=167.78 GlobalRate=167.77 Time=Thu Jul  9 18:58:53 2020\n",
      "[xla:0](20) Loss=0.80982 Rate=1605.68 GlobalRate=1615.16 Time=Thu Jul  9 18:58:54 2020\n",
      "[xla:6](20) Loss=0.89415 Rate=1283.70 GlobalRate=1573.88 Time=Thu Jul  9 18:58:54 2020\n",
      "[xla:3](20) Loss=0.94553 Rate=1645.69 GlobalRate=1671.22 Time=Thu Jul  9 18:58:54 2020\n",
      "[xla:1](20) Loss=1.14740 Rate=1777.64 GlobalRate=1643.49 Time=Thu Jul  9 18:58:54 2020\n",
      "[xla:2](20) Loss=0.88121 Rate=1925.72 GlobalRate=1691.26 Time=Thu Jul  9 18:58:54 2020\n",
      "[xla:4](20) Loss=0.98993 Rate=1557.65 GlobalRate=1658.92 Time=Thu Jul  9 18:58:54 2020\n",
      "[xla:5](20) Loss=0.91852 Rate=1735.43 GlobalRate=1691.09 Time=Thu Jul  9 18:58:54 2020\n",
      "[xla:7](20) Loss=1.02112 Rate=1573.79 GlobalRate=1627.11 Time=Thu Jul  9 18:58:54 2020\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Start training processes\n",
    "def _mp_fn(rank, flags):\n",
    "  global FLAGS\n",
    "  FLAGS = flags\n",
    "  torch.set_default_tensor_type('torch.FloatTensor')\n",
    "  accuracy, data, pred, target = train_resnet18()\n",
    "  args = data.cpu(), pred.cpu(), target.cpu()\n",
    "  if rank == 0:\n",
    "    # Retrieve tensors that are on TPU core 0 and plot.\n",
    "    plot_results(*args)\n",
    "\n",
    "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS['num_cores'],\n",
    "          start_method='fork')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rroH9yiAn-XE"
   },
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#The line above is necesary to show Matplotlib's plots inside a Jupyter Notebook\n",
    "\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#Import image\n",
    "img = cv2.imread(RESULT_IMG_PATH, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "#Show the image with matplotlib\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "PyTorch/TPU ResNet18/CIFAR10 Training",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
